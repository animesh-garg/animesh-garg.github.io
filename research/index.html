<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Animesh Garg Home | Research Projects</title>
  <meta name="description" content="Assistant Professor of Robotics and Machine Learning
">

  

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/research/">


<!--   

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-37485716-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-37485716-1');
      -->

      <!-- Note: plugin scripts must be included after the tracking snippet. -->
      <!--
      <script src="https://ipmeta.io/plugin.js"></script>
      <script>
         provideGtagPlugin({
            apiKey: '4b2663bcfa81585f98c2c92f88994bb2de38e9149a3ea81a5922c4669f9d66d5',
            serviceProvider: 'dimension1',
            networkDomain: 'dimension2',
            networkType: 'dimension3',
         });
      </script> 
      
    </script>-->
  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

<!--     
    <span class="site-title">
        
        <a class="page-link" href="/"><strong>Animesh</strong> Garg</a>
    </span>
     -->

    <a class="page-link" href="http://www.pair.toronto.edu/"><img alt="PAIR Logo" src="/assets/img/pair-logo.png" height="40" style="margin-top:5px"></a>

<!--     <span class="site-title">
      
      <a class="page-link" href="/"><strong>Animesh</strong> Garg</a>
    </span> -->

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <!-- <a class="page-link" href="/">about</a> -->

        <!-- Publications -->
        <!-- <a class="page-link" href="/publications/">publications</a> -->
        <!-- <a class="page-link" href="http://www.pair.toronto.edu/publications/">papers</a> -->

        <!-- research group -->
        <!-- <a class="page-link" href="http://www.pair.toronto.edu">PAIR-lab</a> -->
        
        <!-- PI -->
        <a class="page-link" href="/">Garg (PI)</a>

        <!-- Publications -->
        <a class="page-link" href="http://www.pair.toronto.edu/publications/">papers</a>

        <!-- People -->
        <a class="page-link" href="http://www.pair.toronto.edu/people/">people</a>

        <!-- research -->
        <a class="page-link" href="http://www.pair.toronto.edu/projects/">research</a>
        <!-- <a class="page-link" href="/research/">research</a> -->

        <!-- teaching -->
        <!-- <a class="page-link" href="/teaching/">teaching</a> -->
        <a class="page-link" href="http://www.pair.toronto.edu/teaching/">courses</a>

        <!-- outreach -->
        <a class="page-link" href="http://www.pair.toronto.edu/openings/">join us</a>

        <!-- Blog -->
        <a class="page-link" href="http://www.pair.toronto.edu/blog/">blog</a>

        <!-- Press/News -->
        <!-- <a class="page-link" href="http://www.pair.toronto.edu/news/">press</a> -->
        <a class="page-link" href="http://www.pair.toronto.edu/news/"><i class="far fa-newspaper"></i></a>

        <!-- contact -->
        <!-- <a class="page-link" href="/contact/">contact</a> -->
        <a class="page-link" href="/contact/"><i class="fas fa-paper-plane"></i></a>        

        <!-- Pages -->
<!--         
          
        
          
            
              <a class="page-link" href="/contact/">contact</a>
              
          
        
          
        
          
        
          
              
          
        
          
            
              <a class="page-link" href="/publications/">publications</a>
              
          
        
          
            
              <a class="page-link" href="/research/">Research Projects</a>
              
          
        
          
            
              <a class="page-link" href="/teaching/">teaching</a>
              
          
        
          
        
 -->
        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/garg-cv.pdf">cv</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Research Projects</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Research Projects clearfix">
    <p>We aim to build algorithms for perceptual representations learned by and for interaction, causal understanding of mechanisms, and physically-grounded reasoning in practical settings. An emblematic north star is to enable an autonomous robot to watch an instructional video, or a set of these videos, and then learn a policy to execute the task in a new setting. We build both algorithms and systems that have a broad range of applications in different domains in robot autonomy.
PAIR group blends ideas in <em>Causality</em>, <em>Perception</em>, and <em>Reinforcement Learning</em> towards this vision.</p>

<div class="img_row">
    <!-- <img class="col one left" src="/assets/img/res-ntp_robust.gif" alt="" title="NTP"/> -->
    <img class="col one left" src="/assets/img/res-planning_robot1-small.gif" alt="" title="NTP" />
    <img class="col one left" src="/assets/img/res-cavin.gif" alt="" title="CAVIN" />
    <img class="col one left" src="/assets/img/res-multimodal-test.gif" alt="" title="example image" />
</div>
<div class="col three caption">
    Personal &amp; Service Robotics
</div>

<div class="img_row">
    <img class="col one left" src="/assets/img/res-cutting.gif" alt="" title="Cutting" />
    <img class="col one left" src="/assets/img/res-suturing.gif" alt="" title="Suturing" />    
    <img class="col one left" src="/assets/img/res-acubot.gif" alt="" title="Acubot" />
</div>
<div class="col three caption">
    Surgical &amp; Healthcare Robotics
</div>

<div class="img_row">
    <img class="col one left" src="/assets/img/res-laikago-skateboard.gif" alt="" title="robot-skateboard" />
    <img class="col one left" src="/assets/img/res-laikago-dr.gif" alt="" title="Domain Randomization" />
    <img class="col one left" src="/assets/img/res-laikago-kick.gif" alt="" title="example image" />
</div>
<div class="col three caption">
    Legged Locomotion
</div>

<h3 id="1-generalizable-representations-in-rl-for-robotics">1. Generalizable Representations in RL for Robotics</h3>

<p>A key focus of our work is to understand the role of representations in RL towards efficiency and generalization in skill acquisition. RL is mainly composed of State Space (Input), Action space (Output), a Learning Rule, and Policy (or value) model.</p>

<p>Structured biases upend contemporary methods in all four dimensions, pointing to a need for deeper analysis of representations in RL.<br />
States: <a href="https://sites.google.com/view/task-oriented-grasp">Task Oriented Grasping</a>, <a href="https://sites.google.com/view/task-oriented-grasp">Affordance for Tool-Use</a>, <a href="https://sites.google.com/view/visionandtouch">Making Sense of Touch and Vision</a><br />
Actions: <a href="https://arxiv.org/abs/1906.08880">VICES</a>, <a href="https://www.pair.toronto.edu/laser">LASER</a><br />
Algorithms: <a href="https://arxiv.org/abs/2011.12363">C-Learning</a><br />
Architectures: <a href="https://sites.google.com/view/d2rl/home">Deep-Dense nets in RL</a></p>

<div class="img_row">
    <img class="col one left" src="/assets/img/res-tog.png" style="object-fit: contain;" alt="" title="example image" />
    <img class="col one left" src="/assets/img/res-laser-intro.jpg" style="object-fit: contain;" alt="" title="example image" />
    <img class="col one left" src="/assets/img/res-clearning.gif" style="object-fit: contain;" alt="" title="example image" />
</div>

<ul>
  <li>3D Vision: Object and Scene representations for manipulation.</li>
  <li>Perceptual Concept Learning</li>
  <li>Geomteric Deep Learning for discovery of symmetries</li>
</ul>

<h3 id="2-causal-discovery-and-inference-in-robotics">2. Causal Discovery and Inference in Robotics</h3>

<p>Causal understanding is one of key pillars of my current and future agenda. A simulator is a generative world model, and similarly follows a system of structural mechanisms. However, model learning focuses solely on statistical dependence, while Causal Models go beyond it to build representations that support intervention, planning, and modular reasoning. These methods provide a concrete step towards bridging vision and robotics through sub-goal inference and counterfactual imagination.</p>

<div class="img_row">
    <img class="col two left" src="/assets/img/res-vcdn.gif" style="object-fit: contain;" alt="" title="example image" />
    <img class="col one left" src="/assets/img/res-acgn.gif" alt="" title="example image" />
</div>

<ul>
  <li>Disentangled Generative Models: <a href="https://sites.google.com/nvidia.com/semi-stylegan">Semi-Supervised StyleGAN</a>, <a href="https://github.com/NVIDIA/UnsupervisedLandmarkLearning">Unsupervised Keypoints</a></li>
  <li>Causal Factor Graphs: <a href="https://yunzhuli.github.io/V-CDN/">Visual Causal Discovery</a></li>
  <li>Instruction Guided Counterfactual Generation: <a href="https://iclr-acgn.github.io/ACGN/">Action Concepts</a></li>
</ul>

<h3 id="3-crowd-scale-robot-learning-with-offlinebatch-rl">3. Crowd-Scale Robot Learning with Offline/Batch RL</h3>

<p>Data-driven methods help RL in exploration and reward specification. Robot learning, however, is limited by modest-sized real data. 
Access to data brings new algorithmic opportunities to robotics, as it did in vision and language. However, it also poses challenges due to static nature of data and covariate shifts.</p>

<div class="img_row">
    <img class="col one left" src="/assets/img/res-rt-alps.gif" alt="" title="example image" />
    <img class="col one left" src="/assets/img/res-roboturk.png" style="object-fit: contain;" alt="" title="Roboturk" />
    <img class="col one left" src="/assets/img/res-coda.png" style="object-fit: contain;" alt="" title="example image" />    
</div>

<ul>
  <li>Scalable Teleoperation with Roboturk: <a href="http://roboturk.stanford.edu/">Roboturk-v1</a>, <a href="https://roboturk.stanford.edu/realrobotdataset#tasks">Roboturk-v2</a></li>
  <li>Offline/Batch Policy Learning and Causal Data Augmengation: <a href="https://sites.google.com/stanford.edu/iris/">IRIS</a>, <a href="https://arxiv.org/abs/2007.02863">CoDA</a></li>
  <li>Safe Transfer to Real Systems: <a href="https://stanfordvl.github.io/ARPL/">Adversarial Policy Learning</a>, <a href="https://arxiv.org/abs/1707.04674">Adaptive Polict Transfer</a>, <a href="https://sites.google.com/view/conservative-safety-critics/home">Conservative Safety Critics</a></li>
</ul>

<h3 id="4-structured-biases-for-hierarchical-planning">4. Structured Biases for Hierarchical Planning</h3>

<p>Procedural reasoning, such as in robotics, needs both skills and their structured composition for interaction planning towards a higher-order objective. 
However, manual composition of skills via a finite state-machine design is both tedious and unscalable. Thus the need for inductive bias is intensified for cognitive reasoning. I have developed imitation guided policy learning in abstract spaces for hierarchically structure tasks.</p>

<div class="img_row">
    <img class="col three left" src="/assets/img/res-ntp-small.gif" style="object-fit: contain;" alt="" title="Neural Task Programming" />
    <!-- <img class="col one left" src="/assets/img/res-finding-it.png" alt="" title="Finding It"/>     -->
</div>

<ul>
  <li>Neural Planning Modules for One-Shot Imitation: <a href="https://stanfordvl.github.io/ntp/">NTP</a>, <a href="https://arxiv.org/abs/1807.03480">NTG</a>, <a href="https://arxiv.org/abs/1908.06769">Continuous Symbolic Planner</a></li>
  <li>Task Structure Representations: <a href="https://www.youtube.com/watch?time_continue=2&amp;v=L561cJh7DLE">Transition State Clustering</a>, <a href="http://berkeleyautomation.github.io/tsc-dl/">TSC-DL</a>, <a href="https://animesh.garg.tech/assets/pdf/garg_swirl_ijrr18.pdf">SWIRL</a></li>
  <li>Learning from Videos: <a href="https://finding-it.github.io/">Finding-it</a></li>
  <li>Dynamics with Latent Hierarchical Structure: <a href="http://pair.stanford.edu/cavin/">CAVIN</a></li>
</ul>

<h3 id="5-applications-to-real-robot-systems">5. Applications to Real Robot Systems</h3>

<p>The algorithmic ideas have been motivated by problems in mobility and manipulation in robotics, and have been evaluated on various physical robot platforms.</p>
<div class="img_row">
    <img class="col three left" src="/assets/img/real-robots-garg.png" style="object-fit: contain;" alt="" title="Research Overview" />
</div>

<ul>
  <li>Personal &amp; Service Robotics: <a href="https://sites.google.com/view/task-oriented-grasp">Tool Use</a>, <a href="https://www.youtube.com/watch?v=OdqJuvAHvGE">Task Planning</a>, <a href="https://www.youtube.com/watch?v=NwMukXa8kys&amp;feature=youtu.be">Assembly</a>, <a href="https://ai.stanford.edu/mech-search/multistep">Pick &amp; place</a>, <a href="https://roboturk.stanford.edu/realrobotdataset#tasks">Laundry Layout</a></li>
  <li>Surgical &amp; Healthcare: <a href="https://youtu.be/beVWB6NtAaA">Debridement</a>, <a href="https://youtu.be/z1ehShXFToc">Suturing</a>, <a href="https://youtu.be/l6gQg2VbGcc">Cutting</a>, <a href="https://www.youtube.com/watch?v=YiPq9t0tR3U">Extraction</a>, <a href="https://www.youtube.com/watch?v=Kk_wHiu8nGg&amp;feature=youtu.be">Radiotherapy</a></li>
  <li>Legged Robotics: <a href="https://news.developer.nvidia.com/contact-adaptive-controller-locomotion">Contact Planning</a>, <a href="https://www.pair.toronto.edu/understanding-dr">Domain Randomization</a></li>
</ul>

<!-- 
My work can broadly be divided into topics as follows:





<div class="project ">
    <div class="thumbnail">
        <a href="/projects/1_project/">
        
        <img class="thumbnail" src="/assets/img/12.jpg"/>
            
        <span>
            <h1>Project 1</h1>
            <br/>
            <p>a project with a background image</p>
        </span>
        </a>
    </div>
</div>







<div class="project ">
    <div class="thumbnail">
        <a href="/projects/2_project/">
        
        <img class="thumbnail" src="/assets/img/2.jpg"/>
            
        <span>
            <h1>Project 2</h1>
            <br/>
            <p>a project with a background image</p>
        </span>
        </a>
    </div>
</div>






<div class="project">
    <div class="thumbnail">
        <a href="https://unsplash.com" target="_blank">
        
        <div class="thumbnail blankbox"></div>
            
        <span>
            <h1>Project 3</h1>
            <br/>
            <p>a project that redirects to another website</p>
        </span>
        </a>
    </div>
</div>






<div class="project ">
    <div class="thumbnail">
        <a href="/projects/4_project/">
        
        <div class="thumbnail blankbox"></div>
            
        <span>
            <h1>Project 4</h1>
            <br/>
            <p>another without an image</p>
        </span>
        </a>
    </div>
</div>







<div class="project ">
    <div class="thumbnail">
        <a href="/projects/5_project/">
        
        <img class="thumbnail" src="/assets/img/1.jpg"/>
            
        <span>
            <h1>Project 5</h1>
            <br/>
            <p>a project with a background image</p>
        </span>
        </a>
    </div>
</div>







<div class="project ">
    <div class="thumbnail">
        <a href="/projects/6_project/">
        
        <div class="thumbnail blankbox"></div>
            
        <span>
            <h1>Project 6</h1>
            <br/>
            <p>a project with no image</p>
        </span>
        </a>
    </div>
</div>




 -->

  </article>

  

  

</div>
      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2021 Animesh Garg.
    
    
  </div>

</footer>
  

    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load lighbox JS -->
<script type="text/javascript" src="/assets/js/lightbox.js"></script>
<link rel="stylesheet" href="/assets/css/lightbox.css">



<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX//katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX//katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">


<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-37485716-1', 'auto');
ga('send', 'pageview');
</script>

<script src="https://ipmeta.io/plugin.js"></script>
<script>
 provideGtagPlugin({
    apiKey: '4b2663bcfa81585f98c2c92f88994bb2de38e9149a3ea81a5922c4669f9d66d5',
    serviceProvider: 'dimension1',
    networkDomain: 'dimension2',
    networkType: 'dimension3',
 });
</script>




  </body>

</html>
